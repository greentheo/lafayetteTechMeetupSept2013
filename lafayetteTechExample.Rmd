<!--Import Bootstrap and jquery-->
  <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
  <link href="css/bootstrap.fluidlayout.css" rel="stylesheet" media="screen">
  <script src="http://code.jquery.com/jquery-latest.js"></script>
  <script src="js/bootstrap.min.js"></script>  


<!-- Stuff for Rcharts -->
  <link rel='stylesheet' href='../rCharts/nvd3/css/nv.d3.css'>
  <link rel='stylesheet' href='../rCharts/nvd3/css/rNVD3.css'>
  <script src='../rCharts/nvd3/js/jquery-1.8.2.min.js' type='text/javascript'></script>
  <script src='../rCharts/nvd3/js/d3.v2.min.js' type='text/javascript'></script>
  <script src='../rCharts/nvd3/js/nv.d3.js' type='text/javascript'></script>
  <script src='../rCharts/nvd3/js/fisheye.js' type='text/javascript'></script>




```{r echo=FALSE, results='asis', autodep=TRUE}
library(RMySQL, quietly=T)
library(rCharts,quietly=T)
library(tm, quietly=T)
library(parallel,quietly=T)
library(RCurl,quietly=T)
library(XML,quietly=T)
library(Hmisc,quietly=T)
library(RJSONIO, quietly=T)
source('~/githubrepo/RPackages//commonFuns.R')
source('~/githubrepo/RPackages/helperHTML.R')  
source('htmlToText.R')

```

<!--Top of page NavBar-->
<div class="navbar navbar-fixed-top">
  <div class="navbar-inner">
    <a class="brand" href="http://royaltyAnalytics.com">Royalty Analytics</a>
    <ul class="nav">
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#DataSummary">Data Summary</a></li>
      <li><a href="#Analysis">Analysis</a></li>
      <li><a href="#Results">Results</a></li>
    </ul>
  </div>
</div>




<div class="container">

<div class="row-fluid">

<div class="span12">

<div></div>

<h1>The Greatest Lead List Generation Tool recommendation system analysis.</h1>
<hr>


<h2>Authors</h2>


<h3>Theodore Van Rooy on Behalf of Royalty Analytics</h3>
  
<div class="leaderboard">

  <a name="introduction"></a>
  
  <h1>Introduction</h1>
  
  <p>Royalty Analytics wants to make better recommendations for it's client's business 2 business sales.  This analysis shows how effective the proposed algorithm is.<p>
</div>  

<a name="DataSummary"></a>


Data Summary
------------------------

### Data Head

We will be using data drawn out of Royalty Analytics Lead Generation tool.  The data consists of a number of self selected leads which are then crawled, and cleaned.   

The following is the head of a table giving lead information.

```{r echo=FALSE,  results='asis'}
  con <- dbConnect(MySQL(), user="user", password="password", 
    dbname="munkey96_leadList", host=as.character("scroggles.com"), port=3306)
  leadTable = dbReadTable(con, "leads")
  prettyPrintMat(
    head(leadTable[,c("Name", "Address")]), "Row"
    )
#   dbDisconnect(con)
```


###  Leads by City

Lets take a quick look at leads by city, with a sweet plot.

```{r echo=FALSE, autodep=TRUE}
  leadsCity = lapply(strsplit(leadTable[,"Address"], ","), function(x){
    return(x[2])
  })
  leadsDF = as.data.frame(unlist(leadsCity))
  colnames(leadsDF) = "city"
#   leadsCityTable = table(unlist(leadsCity))
#   leadsDF = as.data.frame(cbind(leadsCityTable, names(leadsCityTable)))
#   colnames(leadsDF) = c("Count", "City")
  p = nPlot(~city, data = leadsDF, type="multiBarChart", id="leadsCity")

```

```{r echo=FALSE, results='asis', autodep=TRUE}
p$print("leadsCity")
```


```{r echo=FALSE}

# dates=as.POSIXct(seq.Date(from=as.Date("2009-01-01"), to=as.Date("2012-12-31"), by="1 day"))
# data = seq(1000, 1500, length.out=length(dates))
# tpi = c(rep(seq(0,2*pi,length.out=365),4),0)
# data = data+100*sin(tpi)
# data = data*(rnorm(n=length(data),1,.03))
# #plot(xts(data, order.by=dates))
# #p=qplot(dates, data,geom="line")
# dataF = as.data.frame(cbind(dates=c(1:length(data)), data=data))
# p = nPlot(data~dates, data = as.data.frame(dataF), type="lineChart", id="data")
# #p$print("chart3")
# x=c(rep(c(1:365), 4), 1)
# d=1:length(x)
# model=lm(data~sin(tpi)+d+x)
# #p+ geom_line(aes(dates, (model$fitted.values)),colour="red")

```

```{r echo=FALSE, results='asis'}
# p$print("data")
```



###  Fetched Data

Fetching the dataset in parallel:

```{r echo=FALSE, cache=TRUE, autodep=TRUE}
  
  slavePrepFun = function(){
    library(RCurl,quietly=T)
    library(tm,quietly=T)
    library(XML,quietly=T)
  }
  
  cl = makeCluster(rep("localhost", 2))
  clusterExport(cl, varlist=c("slavePrepFun", "htmlToText"))
  clusterCall(cl, fun=slavePrepFun)
  
  #apply a fetching operation to each url in the leadTable
  timeSingle = Sys.time()
  pages = lapply(leadTable[leadTable[,"Status"]=="Active","Web"], function(url){
    return(htmlToText(url))
  })
  timeSingle = Sys.time()-timeSingle
  cat('Time to fetch and process pages on single processor:', timeSingle, '\n')

  timeDouble = Sys.time()
  pages = clusterApplyLB(cl, leadTable[leadTable[,"Status"]=="Active","Web"], function(url){
    return(htmlToText(url))
  })
  timeDouble = Sys.time()-timeDouble
  cat('Time to fetch and process pages on two processors:', timeDouble, '\n')

  stopCluster(cl)

```

Twice as fast when running on 2 cores!  


<a name="Analysis"></a>

Analysis
------------------------

To complete this analysis we looked at some word frequency counts:


```{r echo=FALSE,  results='asis', autodep=TRUE}
#top 10 filtered monograms
#make the corpus from the tm package
pagesCorp = Corpus(VectorSource(unlist(pages)))

#remove stopWords
pagesCorp = tm_map(pagesCorp, removeWords, stopwords("english"))

pagesCorp <- tm_map(pagesCorp, tolower)
#remove stems

#create a term document matrix
dtm <- DocumentTermMatrix(pagesCorp)

#maybe remove keywords that are too frequently used
# findFreqTerms(dtm, 6) #words with frequency greater than 6

#apply an arbitrary function to a column (the words)
# apply(dtm , 2, sum)

#words by percent frequency (total count/#documents)
dtmFreq = apply(dtm, 2, function(x)return(sum(x)/dtm$nrow)) 

# dtmFreq[which(dtmFreq>.2 & dtmFreq<.8)] #words found in more than 20% but less than 80% of documents

#what does the word freq cumulative dist look like?
ePlot = Ecdf(dtmFreq, main="ECDF Term Frequency Counts") 
ePlotDF = as.data.frame(cbind(monoGramTermFreq=ePlot$x, cdf=ePlot$y))
p = nPlot(cdf~monoGramTermFreq, data = ePlotDF, type="lineChart", id="termFreq")

#let's get the ones near the tail end
kw = names(dtmFreq[which(dtmFreq>.7 & dtmFreq<1)])
kwFreq = dtmFreq[which(dtmFreq>.7 & dtmFreq<1)]
kwFreqMat = head(
  cbind(keyword=kw, frequency=kwFreq)[
      sort(kwFreq, decreasing=T,index.return=T)$ix,]
  )

```

####  ECDF Term Frequency Counts

```{r echo=FALSE, results='asis', autodep=TRUE}
p$print("termFreq")
```

We can see both the default R plot and the nvD3 plot here... like the nvD3 plot much better.

This tells us that very few of the words are used with a frequency greater than 1 per document, and a lot of them are used less than 70% of the time.  Let's use the intrestesting ones in the 70%-100% range.

```{r echo=FALSE, results='asis', autodep=TRUE}
rownames(kwFreqMat) = c(1:nrow(kwFreqMat))
prettyPrintMat(kwFreqMat, "Rank")

```

<a name="Results"></a>

Results
------------------------

### How Good are our selected words?

Using our simple list of terms found on the front page of the websites, lets poll Google Places and see what kinds of results we get back.



```{r echo=FALSE, autodep=TRUE}

#from the keywords
res=fromJSON(
      getURL(
        paste("https://maps.googleapis.com/maps/api/place/textsearch/json?sensor=false&query=",paste(kwFreqMat[,"keyword"], sep="",collapse="+") ,"&key=AIzaSyBF1_2N6F8RpD394ZchM0pYHhC_NMPwwpk", sep="", collapse="")
        )
      )

#print resulsts
sapply(res$results, function(x)return(x$name))


```

Okay so the algorithm might need some more work...

</div>
</div>
</div>
